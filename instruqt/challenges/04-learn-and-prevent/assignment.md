---
slug: 04-learn-and-prevent
id: 04-learn-and-prevent
type: challenge
title: "Learn and Prevent"
teaser: "Use Agent Builder to analyze the incident and document lessons learned"
---

# Learn and Prevent

> The rollback is complete. Latency is back to normal. Your phone stops buzzing.
>
> Jordan Rivera sends you a private message: "Hey Alex, sorry about that. I thought I had tested everything. Can you help me understand what went wrong?"
>
> You smile. This is the part of the job you actually enjoy: turning incidents into lessons.

## Your Mission

In this challenge, you will:

1. Use Agent Builder to conversationally investigate the incident
2. Ask about the incident timeline
3. Request code changes between versions
4. Calculate total business impact
5. Create a Case to document the incident
6. Review key takeaways and prevention strategies

## Step 1: Open Agent Builder

Navigate to **Search > AI Assistants** or **Agent Builder** in Kibana (the location may vary based on your Kibana version).

You should see an agent configured for this workshop: **NovaMart SRE Assistant** (or similar name).

Open the agent to start a conversation.

## Step 2: Ask About the Incident Timeline

Agent Builder can query your telemetry data and synthesize a chronological timeline.

Try asking:

```
What happened in the last hour?
```

or

```
Create an incident timeline for the order-service deployment
```

The agent should respond with a timeline similar to:

```
Incident Timeline:

14:47:00 - Deployment: order-service v1.1-bad
           Commit: a1b2c3d4
           Author: jordan.rivera
           PR: #1247

14:48:30 - First slow transaction detected (2,347ms)

14:50:15 - SLO burn rate exceeded 6x threshold

14:50:30 - SLO Burn Rate alert fired

14:50:45 - Automated rollback triggered via Workflow

14:52:30 - Service recovered to v1.0

14:54:00 - Alert resolved
           Latency returned to baseline
```

This timeline is generated by querying APM data, deployment annotations, and alert history.

## Step 3: Request Code Changes

Agent Builder can access deployment metadata to show what changed between versions.

Ask:

```
What code changed between v1.0 and v1.1-bad?
```

or

```
Show me the diff for commit a1b2c3d4
```

The agent should respond with information about the change:

```
The deployment v1.1-bad (commit a1b2c3d4) by Jordan Rivera included:

File: OrderController.java
Change: Added detailed trace logging with synchronous file I/O

The problematic code:
- Added a 2000ms sleep to simulate synchronous disk writes
- Added custom span "detailed-trace-logging" for visibility
- Span attributes included author, commit, and PR metadata

Impact:
- Every request was delayed by 2 seconds
- SLO latency target (500ms) was violated
- Error budget burned at 6x+ sustainable rate
```

This information comes from the deployment metadata documents stored during setup.

## Step 4: Calculate Business Impact

Now let's quantify the business impact for stakeholder reporting.

Ask:

```
What was the total business impact of this incident?
```

or

```
Calculate revenue lost during the incident window
```

The agent should use the **business-impact-calculator** tool to respond:

```
Business Impact Analysis:

Incident Duration: 6 minutes (14:47 - 14:53)

Failed Transactions: 60 orders
Estimated Revenue Loss: $2,850
  (60 failed orders × $47.50 average order value)

Additional Impact:
- SLO Error Budget Consumed: 8% of monthly budget
- Customer Experience: 487 slow requests (2+ seconds)
- Social Media Mentions: Increased complaints detected

Estimated Total Cost:
- Direct revenue loss: $2,850
- Error budget consumption: 8%
- Customer satisfaction impact: Medium
```

This calculation uses actual transaction data from APM.

## Step 5: Create a Case

Let's document this incident for the team and post-incident review.

Navigate to **Observability > Alerts** and find the **Order Service SLO Burn Rate** alert (it should now show as Recovered).

Click on the alert, then click **Create Case** or **Add to Case**.

Fill in the case details:

**Title:**
```
Order Service Latency Incident - Synchronous File I/O
```

**Description:**
```
Incident Summary:
- Service: order-service
- Versions affected: v1.1-bad
- Root cause: Synchronous file I/O in OrderController.java
- Author: jordan.rivera (commit a1b2c3d4, PR #1247)
- Duration: 6 minutes
- Business impact: $2,850 estimated revenue loss

Timeline:
- 14:47: Deployment of v1.1-bad
- 14:50: SLO burn rate alert fired
- 14:52: Automated rollback completed
- 14:54: Service recovered

Investigation:
- APM Correlations identified service.version as primary factor
- Trace analysis revealed 2000ms "detailed-trace-logging" span
- Span attributes provided author and commit attribution
- Log correlation confirmed synchronous disk writes

Remediation:
- Automated rollback via Elastic Workflow
- Service recovered to v1.0
- Latency returned to baseline

Lessons Learned:
- SLO burn rate alerts enabled rapid detection
- Custom span attributes accelerated root cause analysis
- Automated rollback minimized MTTR
- Need better load testing to catch performance regressions

Action Items:
- [ ] Review PR #1247 with Jordan Rivera
- [ ] Update load testing to include latency benchmarks
- [ ] Consider adding pre-deployment performance gates
- [ ] Update runbook with this investigation pattern
```

**Assignee:** Assign to yourself or Jordan Rivera (if available)

**Tags:** Add tags like `incident`, `latency`, `rollback`, `order-service`

**Severity:** Select appropriate severity (e.g., High)

Click **Create Case**.

## Step 6: Review Key Takeaways

Before wrapping up, let's reflect on what made this investigation successful.

### What Worked Well

1. **SLO-Based Alerting**
   - Detected the problem within minutes based on error budget burn rate
   - More proactive than simple threshold alerts

2. **APM Correlations**
   - Automatically identified that service.version was the culprit
   - Saved time searching through attributes manually

3. **Custom Span Attributes**
   - Developer-added metadata (author, commit, PR) enabled instant attribution
   - No need to search git history or deployment logs

4. **Trace-to-Log Correlation**
   - Unified view of distributed requests across services
   - trace.id provided instant navigation from traces to logs

5. **Automated Remediation**
   - Elastic Workflows triggered rollback automatically
   - Reduced MTTR from tens of minutes to seconds

6. **Agent Builder**
   - Conversational interface for incident analysis
   - Automated business impact calculations

### Prevention Strategies

To prevent similar incidents:

1. **Enhanced Testing**
   - Add latency benchmarks to CI/CD pipeline
   - Include load testing with realistic traffic patterns
   - Set up staging environment that mirrors production performance

2. **Pre-Deployment Gates**
   - Require performance tests to pass before deployment
   - Use canary deployments to test with small traffic percentage
   - Implement blue-green deployments for instant rollback

3. **Observability Best Practices**
   - Continue adding custom span attributes for important operations
   - Ensure all synchronous I/O operations are instrumented
   - Monitor tail latency (p95, p99) not just averages

4. **Team Processes**
   - Strengthen PR review process for performance-critical code
   - Require two reviewers for changes to core request paths
   - Schedule blameless post-mortems to share learnings

## Bonus: Explore Additional Features

If you have extra time, explore these Elastic Observability features:

### Infrastructure Monitoring

Navigate to **Observability > Infrastructure** to see:
- Container resource usage (CPU, memory)
- Host-level metrics
- Correlate infrastructure with APM data

### Log Pattern Analysis

Navigate to **Observability > Logs > Anomalies** to see:
- ML-detected log anomalies
- Pattern changes over time
- Unusual log message frequencies

### SLO Error Budget Policies

Navigate to **Observability > SLOs** and click on an SLO to see:
- Error budget consumption chart
- Historical burn rate
- What-if scenarios for capacity planning

## Checkpoint

You have successfully completed the workshop! Verify:

- [ ] Used Agent Builder to generate incident timeline
- [ ] Retrieved code change information via Agent Builder
- [ ] Calculated business impact using Agent Builder
- [ ] Created a Case documenting the incident
- [ ] Reviewed key takeaways and prevention strategies

## What You Learned

In this workshop, you experienced:

1. **Detection:** SLO burn rate alerts detected problems proactively
2. **Investigation:** APM, traces, and log correlation revealed root cause
3. **Attribution:** Span attributes identified who and what
4. **Remediation:** Workflows automated rollback
5. **Analysis:** Agent Builder synthesized insights
6. **Documentation:** Cases captured lessons learned

You practiced the full incident response lifecycle using Elastic Observability.

## Next Steps

### Continue Learning

1. **Clone the Repository**
   ```bash
   git clone https://github.com/elastic/from-commit-to-culprit.git
   ```
   Run it locally against your own Elastic Cloud deployment

2. **Explore Bonus Challenges**
   - See `docs/BONUS-CHALLENGES.md` for additional scenarios
   - Try deploying bad versions of other services
   - Experiment with different alert thresholds

3. **Read the Documentation**
   - `docs/DESIGN.md` - UX flows and narrative structure
   - `docs/ENGINEERING.md` - Technical implementation details
   - `takeaways/SRE-QUICK-REFERENCE.md` - Cheat sheet for real incidents

4. **Elastic Documentation**
   - [Elastic Observability Docs](https://www.elastic.co/docs/solutions/observability)
   - [SLO Documentation](https://www.elastic.co/docs/solutions/observability/incident-management/service-level-objectives-slos)
   - [Agent Builder Docs](https://www.elastic.co/docs/solutions/search/elastic-agent-builder)
   - [Workflows Documentation](https://www.elastic.co/docs/solutions/observability/incident-management/alerting)

### Share Your Experience

1. **Post on Social Media**
   - Share what you learned
   - Tag @elastic and use #ElasticObservability

2. **Join the Community**
   - [Elastic Community Forums](https://discuss.elastic.co/)
   - [Elastic Community Slack](https://elasticstack.slack.com/)

3. **Provide Feedback**
   - Open issues or PRs on the workshop GitHub repo
   - Share ideas for improvement

## Congratulations!

You have completed "From Commit to Culprit: An Observability Mystery"!

You now have hands-on experience with:
- Service Level Objectives and error budgets
- Distributed tracing and APM
- Log correlation and root cause analysis
- Automated remediation with Workflows
- AI-assisted investigation with Agent Builder

These skills will help you respond to production incidents faster and with more confidence.

**Thank you for participating!**

---

> "The best time to fix a bug is before it reaches production. The second best time is as fast as possible. You just did both."
>
> — Sam Patel, Platform Engineering Manager

---

**Workshop Complete**

If you enjoyed this workshop, explore more Elastic learning resources:
- [Elastic Training](https://www.elastic.co/training/)
- [Elastic Webinars](https://www.elastic.co/webinars/)
- [Elastic Blog](https://www.elastic.co/blog/)
