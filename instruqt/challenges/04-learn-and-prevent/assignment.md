---
slug: 04-learn-and-prevent
id: 04-learn-and-prevent
type: challenge
title: "Learn and Prevent"
teaser: "Use Agent Builder to analyze the incident and document lessons learned"
---

# Learn and Prevent

> The rollback is complete. Latency is back to normal. Your phone stops buzzing.
>
> Jordan Rivera sends you a private message: "Hey Alex, sorry about that. I thought I had tested everything. Can you help me understand what went wrong?"
>
> You smile. This is the part of the job you actually enjoy: turning incidents into lessons.

## Your Mission

In this challenge, you will:

1. Use Agent Builder to conversationally investigate the incident
2. Ask about the incident timeline
3. Request code changes between versions
4. Calculate total business impact
5. Create a Case to document the incident
6. Review key takeaways and prevention strategies

## Step 1: Open Agent Builder

Navigate to **Search > AI Assistants** or **Agent Builder** in Kibana (the location may vary based on your Kibana version).

You should see an agent configured for this workshop: **NovaMart Incident Investigation Assistant**.

> ðŸ“¸ **Screenshot: Agent Builder Interface**
>
> ![Agent Builder](screenshots/04-agent-builder-interface.png)
>
> *Shows the AI Assistants page with the "NovaMart Incident Investigation Assistant" agent visible. The agent card shows its description and the option to start a conversation.*

Open the agent to start a conversation.

## Step 2: Ask About the Incident Timeline

Agent Builder can query your telemetry data and synthesize a chronological timeline.

The agent has access to 8 custom investigation tools plus 7 built-in Elastic tools. Try asking specific questions that leverage these tools:

**Service Health:**
```
What's the current health of the order-service?
```

**Deployment History:**
```
Show me recent deployments
```

**Incident Timeline:**
```
Build an incident timeline for order-service over the last hour
```

The agent should respond with a timeline similar to:

```
Incident Timeline:

14:47:00 - Deployment: order-service v1.1-bad
           Commit: a1b2c3d4
           Author: jordan.rivera
           PR: #1247

14:48:30 - First slow transaction detected (2,347ms)

14:50:15 - SLO burn rate exceeded 6x threshold

14:50:30 - SLO Burn Rate alert fired

14:50:45 - Automated rollback triggered via Workflow

14:52:30 - Service recovered to v1.0

14:54:00 - Alert resolved
           Latency returned to baseline
```

This timeline is generated by querying APM data, deployment annotations, and alert history.

> ðŸ“¸ **Screenshot: Agent Builder Conversation**
>
> ![Agent Conversation](screenshots/04-agent-conversation.png)
>
> *Shows a conversation with the agent where the user asked about the incident timeline. The agent's response includes a formatted timeline with timestamps, and shows which tools were used (e.g., incident-timeline, deployment-timeline) to gather the information.*

## Step 3: Request Code Changes

Agent Builder can access deployment metadata to show what changed between versions.

Ask:

```
What code changes were in that deployment?
```

or

```
Show me the code diff from the most recent deployment
```

The agent should use the **deployment-code-changes** tool and respond with details about the change:

```
Deployment: v1.1-bad
Service: order-service
Commit: a1b2c3d4
Author: jordan.rivera
PR: PR-1247

File Modified: src/main/java/com/novamart/order/service/OrderService.java
Method: createOrder

Code Before:
// Direct order processing
Order order = new Order(request);
orderRepository.save(order);

Code After:
// Connection pool optimization - warm up connections
...
Thread.sleep(2000);  // THE BUG - unnecessary delay
...
Order order = new Order(request);
orderRepository.save(order);

Issue: The Thread.sleep(2000) was intended for testing but was accidentally 
left in production code, causing a 2-second delay on every order request.
```

This information comes from the deployment metadata documents stored during setup.

## Step 4: Calculate Business Impact

Now let's quantify the business impact for stakeholder reporting.

Ask (using actual timestamps from your environment):

```
Calculate the business impact from 06:43 to 07:00 today for order-service
```

or

```
What was the revenue loss during the incident?
```

The agent should use the **business-impact-calculator** tool to respond:

```
Business Impact Analysis:

Time Window: 2025-12-10 06:43:00Z to 07:00:00Z (17 minutes)

Failed Transactions: 127
Estimated Revenue Loss: $6,032.50
  (127 failed orders Ã— $47.50 average order value)

Recommendation: This represents significant revenue loss. The rapid 
automated rollback minimized the impact.
```

This calculation uses actual transaction data from APM traces. The exact numbers will vary based on when you run the workshop and how long the bad deployment was active.

## Step 5: Create a Case

Let's document this incident for the team and post-incident review.

Navigate to **Observability > Alerts** and find the **Order Service SLO Burn Rate** alert (it should now show as Recovered).

Click on the alert, then click **Create Case** or **Add to Case**.

Fill in the case details:

**Title:**
```
Order Service Latency Incident - Synchronous File I/O
```

**Description:**
```
Incident Summary:
- Service: order-service
- Versions affected: v1.1-bad
- Root cause: Synchronous file I/O in OrderController.java
- Author: jordan.rivera (commit a1b2c3d4, PR #1247)
- Duration: 6 minutes
- Business impact: $2,850 estimated revenue loss

Timeline:
- 14:47: Deployment of v1.1-bad
- 14:50: SLO burn rate alert fired
- 14:52: Automated rollback completed
- 14:54: Service recovered

Investigation:
- APM Correlations identified service.version as primary factor
- Trace analysis revealed 2000ms "detailed-trace-logging" span
- Span attributes provided author and commit attribution
- Log correlation confirmed synchronous disk writes

Remediation:
- Automated rollback via Elastic Workflow
- Service recovered to v1.0
- Latency returned to baseline

Lessons Learned:
- SLO burn rate alerts enabled rapid detection
- Custom span attributes accelerated root cause analysis
- Automated rollback minimized MTTR
- Need better load testing to catch performance regressions

Action Items:
- [ ] Review PR #1247 with Jordan Rivera
- [ ] Update load testing to include latency benchmarks
- [ ] Consider adding pre-deployment performance gates
- [ ] Update runbook with this investigation pattern
```

**Assignee:** Assign to yourself or Jordan Rivera (if available)

**Tags:** Add tags like `incident`, `latency`, `rollback`, `order-service`

**Severity:** Select appropriate severity (e.g., High)

Click **Create Case**.

> ðŸ“¸ **Screenshot: Case Created**
>
> ![Case Created](screenshots/04-case-created.png)
>
> *Shows the completed Case with title, description, tags, and linked alert. Cases provide a permanent record for post-incident reviews and can be shared with stakeholders.*

## Step 6: Understanding Agent Builder Tools

The NovaMart Incident Investigation Assistant has access to **15 tools** total:

### Custom Investigation Tools (8)

These ES|QL-based tools were created specifically for this workshop:

1. **service-health-snapshot** - Get current service metrics (latency, throughput, errors)
2. **apm-latency-comparison** - Compare latency before/after a specific timestamp
3. **deployment-timeline** - View deployment history with timestamps
4. **deployment-code-changes** - Retrieve full code diffs from deployments
5. **error-pattern-analysis** - Group and analyze error patterns
6. **business-impact-calculator** - Calculate revenue loss ($47.50 per failed order)
7. **incident-timeline** - Build minute-by-minute incident timelines
8. **slo-status-budget** - Check SLO compliance and error budget status

### Built-in Elastic Tools (7)

These platform tools provide data access and query capabilities:

- **platform.core.search** - Full-text and analytical searches
- **platform.core.execute_esql** - Execute ES|QL queries
- **platform.core.generate_esql** - Generate ES|QL from natural language
- **platform.core.list_indices** - List available indices
- **platform.core.get_index_mapping** - Get index field mappings
- **platform.core.get_document_by_id** - Fetch specific documents
- **platform.core.cases** - Search and retrieve Cases

### How the Agent Uses Tools

When you ask a question, the agent:
1. Determines which tool(s) can answer your question
2. Calls the appropriate tool(s) with parameters
3. Synthesizes the results into a natural language response

You can see which tools were used in the conversation history.

## Step 7: Review Key Takeaways

Before wrapping up, let's reflect on what made this investigation successful.

### What Worked Well

1. **SLO-Based Alerting**
   - Detected the problem within minutes based on error budget burn rate
   - More proactive than simple threshold alerts

2. **APM Correlations**
   - Automatically identified that service.version was the culprit
   - Saved time searching through attributes manually

3. **Custom Span Attributes**
   - Developer-added metadata (author, commit, PR) enabled instant attribution
   - No need to search git history or deployment logs

4. **Trace-to-Log Correlation**
   - Unified view of distributed requests across services
   - trace.id provided instant navigation from traces to logs

5. **Automated Remediation**
   - Elastic Workflows triggered rollback automatically
   - Reduced MTTR from tens of minutes to seconds

6. **Agent Builder**
   - Conversational interface for incident analysis
   - Automated business impact calculations

### Prevention Strategies

To prevent similar incidents:

1. **Enhanced Testing**
   - Add latency benchmarks to CI/CD pipeline
   - Include load testing with realistic traffic patterns
   - Set up staging environment that mirrors production performance

2. **Pre-Deployment Gates**
   - Require performance tests to pass before deployment
   - Use canary deployments to test with small traffic percentage
   - Implement blue-green deployments for instant rollback

3. **Observability Best Practices**
   - Continue adding custom span attributes for important operations
   - Ensure all synchronous I/O operations are instrumented
   - Monitor tail latency (p95, p99) not just averages

4. **Team Processes**
   - Strengthen PR review process for performance-critical code
   - Require two reviewers for changes to core request paths
   - Schedule blameless post-mortems to share learnings

## Troubleshooting Agent Builder

If the agent doesn't respond as expected:

**Agent not found:**
- Verify you're in the correct Kibana space
- Check: Search > AI Assistants
- Look for "NovaMart Incident Investigation Assistant"

**Tools return no data:**
- Ensure services are running and generating traces
- Check that the load generator is active
- Verify you're using recent timestamps in your queries

**Unexpected responses:**
- Try rephrasing your question
- Be specific about service names (e.g., "order-service")
- Include timestamps in ISO format (e.g., "2025-12-10T06:43:00Z")

**Example questions that work well:**
```
What's the current health of the order-service?
Show me recent deployments
What code changes were in that deployment?
Calculate business impact from [start] to [end] for order-service
Build an incident timeline for order-service over the last hour
```

## Bonus: Explore Additional Features

If you have extra time, explore these Elastic Observability features:

### Infrastructure Monitoring

Navigate to **Observability > Infrastructure** to see:
- Container resource usage (CPU, memory)
- Host-level metrics
- Correlate infrastructure with APM data

### Log Pattern Analysis

Navigate to **Observability > Logs > Anomalies** to see:
- ML-detected log anomalies
- Pattern changes over time
- Unusual log message frequencies

### SLO Error Budget Policies

Navigate to **Observability > SLOs** and click on an SLO to see:
- Error budget consumption chart
- Historical burn rate
- What-if scenarios for capacity planning

## Checkpoint

You have successfully completed the workshop! Verify:

- [ ] Used Agent Builder to generate incident timeline
- [ ] Retrieved code change information via Agent Builder
- [ ] Calculated business impact using Agent Builder
- [ ] Created a Case documenting the incident
- [ ] Reviewed key takeaways and prevention strategies

## What You Learned

In this workshop, you experienced:

1. **Detection:** SLO burn rate alerts detected problems proactively
2. **Investigation:** APM, traces, and log correlation revealed root cause
3. **Attribution:** Span attributes identified who and what
4. **Remediation:** Workflows automated rollback
5. **Analysis:** Agent Builder synthesized insights
6. **Documentation:** Cases captured lessons learned

You practiced the full incident response lifecycle using Elastic Observability.

## Next Steps

### Continue Learning

1. **Clone the Repository**
   ```bash
   git clone https://github.com/leungsteve/elastic-trace-the-culprit.git
   ```
   Run it locally against your own Elastic Cloud deployment

2. **Explore Bonus Challenges**
   - See `docs/BONUS-CHALLENGES.md` for additional scenarios
   - Try deploying bad versions of other services
   - Experiment with different alert thresholds

3. **Read the Documentation**
   - `docs/DESIGN.md` - UX flows and narrative structure
   - `docs/ENGINEERING.md` - Technical implementation details
   - `takeaways/SRE-QUICK-REFERENCE.md` - Cheat sheet for real incidents

4. **Elastic Documentation**
   - [Elastic Observability Docs](https://www.elastic.co/docs/solutions/observability)
   - [SLO Documentation](https://www.elastic.co/docs/solutions/observability/incident-management/service-level-objectives-slos)
   - [Agent Builder Docs](https://www.elastic.co/docs/solutions/search/elastic-agent-builder)
   - [Workflows Documentation](https://www.elastic.co/docs/solutions/observability/incident-management/alerting)

### Share Your Experience

1. **Post on Social Media**
   - Share what you learned
   - Tag @elastic and use #ElasticObservability

2. **Join the Community**
   - [Elastic Community Forums](https://discuss.elastic.co/)
   - [Elastic Community Slack](https://elasticstack.slack.com/)

3. **Provide Feedback**
   - Open issues or PRs on the workshop GitHub repo
   - Share ideas for improvement

## Congratulations!

You have completed "From Commit to Culprit: An Observability Mystery"!

You now have hands-on experience with:
- Service Level Objectives and error budgets
- Distributed tracing and APM
- Log correlation and root cause analysis
- Automated remediation with Workflows
- AI-assisted investigation with Agent Builder

These skills will help you respond to production incidents faster and with more confidence.

**Thank you for participating!**

---

> "The best time to fix a bug is before it reaches production. The second best time is as fast as possible. You just did both."
>
> â€” Sam Patel, Platform Engineering Manager

---

**Workshop Complete**

If you enjoyed this workshop, explore more Elastic learning resources:
- [Elastic Training](https://www.elastic.co/training/)
- [Elastic Webinars](https://www.elastic.co/webinars/)
- [Elastic Blog](https://www.elastic.co/blog/)
